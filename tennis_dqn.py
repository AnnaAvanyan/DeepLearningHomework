# -*- coding: utf-8 -*-
"""tennis_DQN.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1IiJkbQqQBDGF_FkVAptUASZaFZAvbPEc
"""

!pip install -r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit2/requirements-unit2.txt

!pip install gymnasium[atari]
!pip install gymnasium[accept-rom-licesnse]

!pip install ale_py

!pip install gymnasium[atari,accept-rom-license]
!pip install ale_py

!pip install gym[atari] gym[accept-rom-license]

# Commented out IPython magic to ensure Python compatibility.
# %load_ext tensorboard
# %tensorboard --logdir /content/drive/MyDrive/tensorboard_logs

import gym
import torch
import torch.nn as nn
import torch.optim as optim
import collections
import numpy as np
from torch.utils.tensorboard import SummaryWriter
from gym.wrappers import RecordVideo

from google.colab import drive
drive.mount('/content/drive')

# # Hyperparameters
# ENV_NAME = "ALE/Tennis-v5"
# GAMMA = 0.99
# ALPHA = 0.2
# LEARNING_RATE = 1e-4
# BATCH_SIZE = 32
# TARGET_UPDATE_FREQ = 1000
# REPLAY_SIZE = 10000
# MIN_REPLAY_SIZE = 1000
# EPSILON_DECAY = 0.9995
# MIN_EPSILON = 0.02
# START_EPSILON = 1.0
# MAX_FRAMES = 50000
VIDEO_SAVE_PATH = "/content/drive/MyDrive/tennis_videos/"

import cv2
import os
from collections import deque
from torch.cuda.amp import GradScaler, autocast
import random

class DQN(nn.Module):
    def __init__(self, input_shape, n_actions):
        super(DQN, self).__init__()
        self.conv = nn.Sequential(
            nn.Conv2d(input_shape[0], 16, kernel_size=8, stride=4),
            nn.ReLU(),
            nn.Conv2d(16, 32, kernel_size=4, stride=2),
            nn.ReLU(),
        )
        conv_out_size = self._get_conv_out(input_shape)
        self.fc = nn.Sequential(
          nn.Linear(conv_out_size, 128),
          nn.ReLU(),
          nn.Linear(128, n_actions)
      )


    def _get_conv_out(self, shape):
        with torch.no_grad():
            o = self.conv(torch.zeros(1, *shape))
        return int(np.prod(o.size()))

    def forward(self, x):
        x = x.float() / 255.0
        conv_out = self.conv(x).view(x.size(0), -1)
        return self.fc(conv_out)


# Hyperparameters
ENV_NAME = "ALE/Tennis-v5"
GAMMA = 0.99
ALPHA = 0.2
BATCH_SIZE = 16
MIN_EPSILON = 0.02
START_EPSILON = 0.5
MAX_FRAMES = 100000
EPSILON_DECAY = 0.9995
LEARNING_RATE = 1e-3
REPLAY_SIZE = 20000
MIN_REPLAY_SIZE = 5000
TARGET_UPDATE_FREQ = 1000
MAX_FRAMES_PER_EPISODE = 1000


class ExperienceReplayBuffer:
    def __init__(self, capacity):
        self.buffer = collections.deque(maxlen=capacity)

    def __len__(self):
        return len(self.buffer)

    def append(self, experience):
        self.buffer.append(experience)

    def sample(self, batch_size):
        experiences = np.random.choice(len(self.buffer), batch_size, replace=False)
        states, actions, rewards, dones, next_states = zip(*(self.buffer[idx] for idx in experiences))
        return (
            np.array(states, dtype=np.uint8),
            np.array(actions),
            np.array(rewards, dtype=np.float32),
            np.array(dones, dtype=bool),
            np.array(next_states, dtype=np.uint8),
        )


class Agent:
    def __init__(self, env, exp_buffer, max_frames_per_episode=1000):
        self.env = env
        self.exp_buffer = exp_buffer
        self.state_buffer = collections.deque(maxlen=4)
        self.max_frames_per_episode = max_frames_per_episode
        self.reset()

    def reset(self):
        state = self.env.reset()
        if isinstance(state, tuple):
            state = state[0]
        state_processed = self.preprocess_observation(state)
        self.state_buffer.clear()
        for _ in range(4):
            self.state_buffer.append(state_processed)
        self.total_reward = 0.0
        self.frames_in_episode = 0

    def preprocess_observation(self, obs):
        gray_obs = cv2.cvtColor(obs, cv2.COLOR_RGB2GRAY)
        resized_obs = cv2.resize(gray_obs, (84, 84), interpolation=cv2.INTER_AREA)
        return resized_obs

    def play_step(self, net, epsilon=0.0, device='cpu'):
        done_reward = None
        state_stack = np.array(self.state_buffer, dtype=np.uint8)
        state_v = torch.tensor(np.array([state_stack]), device=device)

        if np.random.random() < epsilon:
            action = self.env.action_space.sample()
        else:
            with torch.no_grad():
                q_vals = net(state_v)
                _, act_v = torch.max(q_vals, dim=1)
                action = int(act_v.item())

        step_result = self.env.step(action)
        if len(step_result) == 4:
            next_state, reward, done, info = step_result
            is_done = done
        elif len(step_result) == 5:
            next_state, reward, terminated, truncated, info = step_result
            is_done = terminated or truncated
        else:
            raise ValueError("Unexpected number of values returned from env.step()")

        self.total_reward += reward
        self.frames_in_episode += 1

        if self.frames_in_episode >= self.max_frames_per_episode:
            is_done = True

        next_state_processed = self.preprocess_observation(next_state)
        self.state_buffer.append(next_state_processed)
        next_state_stack = np.array(self.state_buffer, dtype=np.uint8)

        exp = (state_stack, action, reward, is_done, next_state_stack)
        self.exp_buffer.append(exp)
        self.state = next_state

        if is_done:
            done_reward = self.total_reward
            print(f"Эпизод завершён с наградой: {self.total_reward}")
            self.reset()

        return done_reward


def calculate_loss(batch, net, target_net, device='cpu'):
    states, actions, rewards, dones, next_states = batch
    states_v = torch.tensor(states, dtype=torch.float32).to(device)
    next_states_v = torch.tensor(next_states, dtype=torch.float32).to(device)
    actions_v = torch.tensor(actions, dtype=torch.int64).to(device)
    rewards_v = torch.tensor(rewards, dtype=torch.float32).to(device)
    done_mask = torch.BoolTensor(dones).to(device)

    state_action_values = net(states_v / 255.0).gather(1, actions_v.unsqueeze(-1)).squeeze(-1)
    with torch.no_grad():
        next_state_values = target_net(next_states_v / 255.0).max(1)[0]
        next_state_values[done_mask] = 0.0
    expected_state_action_values = rewards_v + GAMMA * next_state_values
    loss = nn.MSELoss()(state_action_values, expected_state_action_values)
    return loss


def train():
    if not os.path.exists('/content/drive'):
        from google.colab import drive
        drive.mount('/content/drive')

    VIDEO_SAVE_PATH = "/content/drive/MyDrive/tennis_videos/"
    os.makedirs(VIDEO_SAVE_PATH, exist_ok=True)

    env = gym.make(ENV_NAME, render_mode='rgb_array')
    env = gym.wrappers.RecordEpisodeStatistics(env)

    env = RecordVideo(
        env,
        video_folder=VIDEO_SAVE_PATH,
        episode_trigger=lambda episode_id: episode_id % 10 == 0,
        name_prefix='tennis'
    )
    env.metadata['render_fps'] = 30

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    input_shape = (4, 84, 84)
    n_actions = env.action_space.n
    policy_net = DQN(input_shape, n_actions).to(device)
    target_net = DQN(input_shape, n_actions).to(device)
    target_net.load_state_dict(policy_net.state_dict())

    buffer = ExperienceReplayBuffer(REPLAY_SIZE)
    agent = Agent(env, buffer, max_frames_per_episode=MAX_FRAMES_PER_EPISODE)
    optimizer = optim.Adam(policy_net.parameters(), lr=LEARNING_RATE)

    epsilon = START_EPSILON
    total_rewards = []
    frame_idx = 0
    best_mean_reward = None
    episode_idx = 0

    try:
        while frame_idx < MAX_FRAMES:
            frame_idx += 1
            reward = agent.play_step(policy_net, epsilon, device=device)
            if reward is not None:
                episode_idx += 1
                total_rewards.append(reward)
                mean_reward = np.mean(total_rewards[-100:])
                print(f"Эпизод {episode_idx}: Награда = {reward:.2f}")
                print(f"{frame_idx}: Завершено {len(total_rewards)} эпизодов, Средняя награда {mean_reward:.3f}, Epsilon {epsilon:.5f}")

                epsilon = max(MIN_EPSILON, epsilon * EPSILON_DECAY)
                print(f"Декрементировал epsilon до: {epsilon:.5f}")

                if best_mean_reward is None or best_mean_reward < mean_reward:
                    torch.save(policy_net.state_dict(), "/content/drive/MyDrive/tennis_videos/best_model.dat")
                    if best_mean_reward is not None:
                        print(f"Обновлена лучшая средняя награда: {best_mean_reward:.3f} -> {mean_reward:.3f}")
                    best_mean_reward = mean_reward

                if mean_reward > 0:
                    print(f"Задача решена за {frame_idx} кадров!")
                    break

            if len(buffer) < MIN_REPLAY_SIZE:
                continue

            if frame_idx % TARGET_UPDATE_FREQ == 0:
                target_net.load_state_dict(policy_net.state_dict())
                print("Обновлена целевая сеть.")

            batch = buffer.sample(BATCH_SIZE)
            optimizer.zero_grad(set_to_none=True)
            loss_t = calculate_loss(batch, policy_net, target_net, device=device)
            loss_t.backward()
            torch.nn.utils.clip_grad_norm_(policy_net.parameters(), max_norm=1.0)
            optimizer.step()

    except KeyboardInterrupt:
        print("Обучение прервано, Сохранение модели...")
        torch.save(policy_net.state_dict(), "/content/drive/MyDrive/tennis_videos/interrupted_model.dat")
    finally:
        env.close()



if __name__ == "__main__":
    train()

# class DQN(nn.Module):
#     def __init__(self, input_shape, n_actions):
#         super(DQN, self).__init__()
#         # Уменьшенная архитектура сети для ускорения обучения
#         self.conv = nn.Sequential(
#             nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4),
#             nn.ReLU(),
#             nn.Conv2d(32, 64, kernel_size=4, stride=2),
#             nn.ReLU(),
#             # При необходимости можно добавить третий сверточный слой
#             # nn.Conv2d(64, 64, kernel_size=3, stride=1),
#             # nn.ReLU()
#         )
#         conv_out_size = self._get_conv_out(input_shape)
#         self.fc = nn.Sequential(
#             nn.Linear(conv_out_size, 512),
#             nn.ReLU(),
#             # Временно убираем Dropout для упрощения
#             nn.Linear(512, n_actions)
#         )

#     def _get_conv_out(self, shape):
#         with torch.no_grad():
#             o = self.conv(torch.zeros(1, *shape))
#         return int(np.prod(o.size()))

#     def forward(self, x):
#         x = x.float() / 255.0
#         conv_out = self.conv(x).view(x.size(0), -1)
#         return self.fc(conv_out)

# # Гиперпараметры:
# ENV_NAME = "ALE/Tennis-v5"
# GAMMA = 0.99
# ALPHA = 0.2
# BATCH_SIZE = 32
# MIN_EPSILON = 0.02
# START_EPSILON = 1.0
# MAX_FRAMES = 500000  # Увеличиваем число кадров для сложной игры
# EPSILON_DECAY = 0.9999  # Увеличиваем скорость снижения epsilon
# LEARNING_RATE = 1e-4     # Снижаем lr для более стабильного обучения
# REPLAY_SIZE = 100000
# MIN_REPLAY_SIZE = 5000
# TARGET_UPDATE_FREQ = 1000

# class ExperienceReplayBuffer:
#     def __init__(self, capacity):
#         self.buffer = collections.deque(maxlen=capacity)

#     def __len__(self):
#         return len(self.buffer)

#     def append(self, experience):
#         self.buffer.append(experience)

#     def sample(self, batch_size):
#         experiences = np.random.choice(len(self.buffer), batch_size, replace=False)
#         states, actions, rewards, dones, next_states = zip(*(self.buffer[idx] for idx in experiences))
#         return (
#             np.array(states, dtype=np.uint8),
#             np.array(actions),
#             np.array(rewards, dtype=np.float32),
#             np.array(dones, dtype=bool),
#             np.array(next_states, dtype=np.uint8),
#         )

# class Agent:
#     def __init__(self, env, exp_buffer):
#         self.env = env
#         self.exp_buffer = exp_buffer
#         self.state_buffer = deque(maxlen=4)
#         self.reset()

#     def reset(self):
#         state = self.env.reset()
#         # Gym vs Gymnasium
#         if isinstance(state, tuple):
#             state = state[0]
#         state_processed = self.preprocess_observation(state)
#         self.state_buffer.clear()
#         for _ in range(4):
#             self.state_buffer.append(state_processed)
#         self.total_reward = 0.0

#     def preprocess_observation(self, obs):
#         gray_obs = cv2.cvtColor(obs, cv2.COLOR_RGB2GRAY)
#         resized_obs = cv2.resize(gray_obs, (84, 84), interpolation=cv2.INTER_AREA)
#         return resized_obs

#     def play_step(self, net, epsilon=0.0, device='cpu'):
#         done_reward = None
#         state_stack = np.array(self.state_buffer, dtype=np.uint8)
#         state_v = torch.tensor(np.array([state_stack]), device=device)

#         # Выбор действия
#         if np.random.random() < epsilon:
#             action = self.env.action_space.sample()
#             # print(f"Выбрано случайное действие: {action}")
#         else:
#             with torch.no_grad():
#                 q_vals = net(state_v)
#                 _, act_v = torch.max(q_vals, dim=1)
#                 action = int(act_v.item())
#                 # print(f"Выбрано действие из Q-значений: {action}")

#         # Шаг в среде
#         step_result = self.env.step(action)
#         # Обработка в зависимости от версии Gym
#         if len(step_result) == 4:
#             next_state, reward, done, info = step_result
#             is_done = done
#         elif len(step_result) == 5:
#             next_state, reward, terminated, truncated, info = step_result
#             is_done = terminated or truncated
#         else:
#             raise ValueError("Unexpected number of values returned from env.step()")

#         self.total_reward += reward

#         # Убираем клиппинг наград, чтобы видеть реальные значения
#         # reward = np.clip(reward, -1, 1)  # Закомментировано

#         next_state_processed = self.preprocess_observation(next_state)
#         self.state_buffer.append(next_state_processed)
#         next_state_stack = np.array(self.state_buffer, dtype=np.uint8)

#         # Добавляем опыт в буфер
#         exp = (state_stack, action, reward, is_done, next_state_stack)
#         self.exp_buffer.append(exp)
#         self.state = next_state
#         if is_done:
#             done_reward = self.total_reward
#             print(f"Эпизод завершён с наградой: {self.total_reward}")
#             self.reset()
#         return done_reward

# def calculate_loss(batch, net, target_net, device='cpu'):
#     states, actions, rewards, dones, next_states = batch
#     states_v = torch.tensor(states, dtype=torch.float32).to(device)
#     next_states_v = torch.tensor(next_states, dtype=torch.float32).to(device)
#     actions_v = torch.tensor(actions, dtype=torch.int64).to(device)
#     rewards_v = torch.tensor(rewards, dtype=torch.float32).to(device)
#     done_mask = torch.BoolTensor(dones).to(device)

#     state_action_values = net(states_v / 255.0).gather(1, actions_v.unsqueeze(-1)).squeeze(-1)
#     with torch.no_grad():
#         next_state_values = target_net(next_states_v / 255.0).max(1)[0]
#         next_state_values[done_mask] = 0.0
#     expected_state_action_values = rewards_v + GAMMA * next_state_values
#     loss = nn.MSELoss()(state_action_values, expected_state_action_values)
#     return loss

# def train():
#     env = gym.make(ENV_NAME, render_mode='rgb_array')
#     env = gym.wrappers.RecordEpisodeStatistics(env)
#     video_folder = './tennis_videos'
#     os.makedirs(video_folder, exist_ok=True)
#     env = RecordVideo(
#         env,
#         video_folder=video_folder,
#         episode_trigger=lambda episode_id: episode_id % 10 == 0,
#         name_prefix='tennis'
#     )
#     env.metadata['render_fps'] = 30

#     device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
#     print("Используемое устройство:", device)

#     input_shape = (4, 84, 84)
#     n_actions = env.action_space.n
#     policy_net = DQN(input_shape, n_actions).to(device)
#     target_net = DQN(input_shape, n_actions).to(device)
#     target_net.load_state_dict(policy_net.state_dict())

#     buffer = ExperienceReplayBuffer(REPLAY_SIZE)
#     agent = Agent(env, buffer)
#     optimizer = optim.Adam(policy_net.parameters(), lr=LEARNING_RATE)

#     epsilon = START_EPSILON
#     total_rewards = []
#     frame_idx = 0
#     best_mean_reward = None
#     episode_idx = 0

#     try:
#         while frame_idx < MAX_FRAMES:
#             frame_idx += 1
#             reward = agent.play_step(policy_net, epsilon, device=device)
#             if reward is not None:
#                 episode_idx += 1
#                 total_rewards.append(reward)
#                 mean_reward = np.mean(total_rewards[-100:])
#                 print(f"Эпизод {episode_idx}: Награда = {reward:.2f}")
#                 print(f"{frame_idx}: Завершено {len(total_rewards)} эпизодов, Средняя награда {mean_reward:.3f}, Epsilon {epsilon:.5f}")
#                 # Уменьшаем epsilon после завершения эпизода
#                 epsilon = max(MIN_EPSILON, epsilon * EPSILON_DECAY)
#                 print(f"Декрементировал epsilon до: {epsilon:.5f}")
#                 if best_mean_reward is None or best_mean_reward < mean_reward:
#                     torch.save(policy_net.state_dict(), "best_model.dat")
#                     if best_mean_reward is not None:
#                         print(f"Обновлена лучшая средняя награда: {best_mean_reward:.3f} -> {mean_reward:.3f}")
#                     best_mean_reward = mean_reward
#                 if episode_idx % 10 == 0:
#                     print(f"Видео для эпизода {episode_idx} сохранено.")
#                 # Для очень сложной игры, возможно, потребуется гораздо больше кадров и эпизодов, чтобы увидеть прогресс.
#                 if mean_reward > 0:
#                     print(f"Задача решена за {frame_idx} кадров!")
#                     break

#             if len(buffer) < MIN_REPLAY_SIZE:
#                 continue

#             if frame_idx % TARGET_UPDATE_FREQ == 0:
#                 target_net.load_state_dict(policy_net.state_dict())
#                 print("Обновлена целевая сеть.")

#             batch = buffer.sample(BATCH_SIZE)
#             optimizer.zero_grad(set_to_none=True)
#             loss_t = calculate_loss(batch, policy_net, target_net, device=device)
#             loss_t.backward()
#             torch.nn.utils.clip_grad_norm_(policy_net.parameters(), max_norm=1.0)
#             optimizer.step()

#     except KeyboardInterrupt:
#         print("Обучение прервано пользователем. Сохранение модели...")
#         torch.save(policy_net.state_dict(), "interrupted_model.dat")
#     finally:
#         env.close()

# if __name__ == "__main__":
#     train()